{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2024 Semester 1\n",
    "\n",
    "## Assignment 1: Wine quality classification with K-NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     `1172598`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "wine_train = pd.read_csv('winequality-train.csv')\n",
    "wine_test = pd.read_csv('winequality-test.csv')\n",
    "\n",
    "def KNN_Classifier(k, df_test, df_train):\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through test data\n",
    "    for index, row in df_test.drop(columns=['quality']).iterrows():\n",
    "        distance_arr = []\n",
    "\n",
    "        # Iterate through training data\n",
    "        for index1, row1 in df_train.drop(columns=['quality']).iterrows():\n",
    "            # Call function that calculates distance and add to distance array\n",
    "            distance_arr.append(Calc_Distance(len(df_test.columns)-1, row, row1))\n",
    "\n",
    "        # Add distance array to dataframe and sort it by distance\n",
    "        df_temp = df_train.assign(distance=distance_arr)\n",
    "        df_temp = df_temp.sort_values('distance')\n",
    "\n",
    "        labels.append(Vote(k, df_temp))\n",
    "    return labels\n",
    "\n",
    "# Function to calculate distance of multi dimensional arrays\n",
    "def Calc_Distance(length, row, row1):\n",
    "    # Value used for distance i.e (a-b)^2\n",
    "    dis = 0\n",
    "\n",
    "    # Iterate through all attributes of both test and training instances\n",
    "    for i in range(length):\n",
    "        # Add each dis value for each attribute for instances from test and training\n",
    "        dis = dis + pow(row.iloc[i] - row1.iloc[i], 2)\n",
    "    \n",
    "    # Return multi dimensional distance\n",
    "    return pow(dis, 1/2)\n",
    "\n",
    "# Conditionals to check voting criteria\n",
    "def Vote(k, df_temp):\n",
    "    if (k == 1):\n",
    "        label = df_temp['quality'].iloc[0]\n",
    "    else:\n",
    "        # Nearest quality predictions, note the distance is already sorted\n",
    "        values = df_temp[['quality', 'distance']][0:k].reset_index()\n",
    "\n",
    "        # Iterate through k values of predicted quality and count good and bad predictions\n",
    "        good = 0\n",
    "        bad = 0\n",
    "        for index, row in values.iterrows():\n",
    "            if (row['quality'] == 0):\n",
    "                bad = bad + 1\n",
    "            else:\n",
    "                good = good + 1\n",
    "        # Check each outcome\n",
    "        if (good == bad):\n",
    "            # This is case where \n",
    "            dups = values['distance'].duplicated().tolist()\n",
    "            # Check if duplicates exits\n",
    "            if True in dups:\n",
    "                count = 0\n",
    "                # Get all duplicates of nearest distance, ignore other duplicates\n",
    "                for index in range(len(dups)):\n",
    "                    if (dups[index] == True):\n",
    "                        if (count == 0):\n",
    "                            start = index - 1\n",
    "                        count = count + 1\n",
    "                        if (index == 9):\n",
    "                            end = start + count\n",
    "                            break\n",
    "                    elif ((dups[index] == False and count != 0) or (dups[index] == True and index == 9)):\n",
    "                        end = start + count\n",
    "                        break\n",
    "                # All nearest duplicates\n",
    "                same_dis = values['quality'][start:end].tolist()\n",
    "\n",
    "                # If duplicates all have same quality just pick 1-NN\n",
    "                if (len(set(same_dis)) == 1):\n",
    "                    label = values['quality'].iloc[0]\n",
    "                # Otherwise pick randomly amongst duplicates\n",
    "                else:\n",
    "                    label = random.choice(same_dis)\n",
    "                    \n",
    "            # Simply pick 1-NN if no duplicates\n",
    "            else:\n",
    "                label = values['quality'].iloc[0]\n",
    "        elif (good > bad):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "    return label\n",
    "    \n",
    "\n",
    "labels = KNN_Classifier(10, wine_test[0:50], wine_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 1-NN classification\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7400\n"
     ]
    }
   ],
   "source": [
    "def Find_Proportion(df_test, labels_classified):\n",
    "    labels_test = df_test['quality'].tolist()\n",
    "    count = 0\n",
    "    for i in range(len(labels_classified)):\n",
    "        if (labels_test[i] == labels_classified[i]):\n",
    "            count = count + 1\n",
    "    return float(count)/float(len(labels_classified))\n",
    "\n",
    "print(\"{:0.4f}\".format(Find_Proportion(wine_test, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalization\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model extensions\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "Compare the performance of your best 1-NN model from Question 3 to a Gaussian naive Bayes model on this dataset (you may use library functions to implement the Gaussian naive Bayes model). In your write-up, state the accuracy of the naive Bayes model and identify instances where the two models disagree. Why do the two models classify these instances differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "Implement two additional distance measures for your K-NN model: cosine similarity and Mahalanobis distance (you may use library functions for these distance measures). Do 1-NN classification using each of these new distance measures and the three normalization options from Question 3. Discuss how the new distance metrics compare to Euclidean distance and how each metric is affected by normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3\n",
    "Implement either of the two K-NN weighting strategies discussed in lecture (inverse linear distance or inverse distance). Compare the performance of the weighted and majority vote models for a few different values of K. In your write-up, discuss how weighting strategy and the value of K affect the model's decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4\n",
    "Measure the empirical distribution of class labels in the training dataset (what percentage of the training data comes from each class). Then evaluate the distribution of labels predicted by your K-NN model for the test data, for a range of values for K. Does the class distribution of the predicted labels match the class distribution of the training data? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
